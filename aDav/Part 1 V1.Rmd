
---
title: Assignment 1
author: "Osaro Orebor (1168827), Calvin Boateng, Mila-Jair Berdenis van Berlekom (6805027)"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: united
---


```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Our Research question is: Which audio features best predict a song’s popularity on Spotify?

We aimed to explore whether certain musical characteristics—such as danceability, loudness, speechiness, and year of release—have a significant effect on a song's popularity. We also compared different modeling approaches (linear regression and Lasso regression) to identify which predictors are most useful and how well these models perform.


# Setting Libraries
These libraries provide us with the tools needed for data preprocessing and analysis. We are using various packages that allow us to explore and visualize the dataset, as well as build models to help answer our research question.

```{r Libraries2, message = FALSE, warning = FALSE}
suppressWarnings({
  library(dplyr)
  library(ggplot2)
  library(ggpubr)
  library(kableExtra)
  library(tidyverse)
  library(readr)
  library(knitr)
  library(weathermetrics)
  library(gridExtra)
  library(plotly)
  library(GGally)
  library(magrittr)
  library(regclass)
  library(MASS)
  library(pROC)
  library(caret)
  library(car)
  library(glmnet)
})
```

```{r set path}
# Set seed for reproducibility
set.seed(123)

```
# Loading in the Data
We load the Spotify dataset which allows us to access the variables required to address the research question. It provides the data needed to explore the relationship between the mentioned features and popularity.

```{r loading in the file, echo=FALSE}
data <- read.csv("data/Spotify-2000.csv")
```
.

# Explorative Data Analysis 

## looking at the data and analyse the variables used
We perform exploratory analysis to inspect the data we're working with. We display the first and last 10 rows, as is verifies that the data was loaded in correctly and gives us a quick overview of the content. We follow up by summarizing the data structure to explore the data types and variables needed for the analysis.

### Summary
```{r Data summary, echo = FALSE}
kable(head(data, 10)) %>% 
  kable_styling("striped", full_width = F) %>% 
  scroll_box(width = "800px", height = "500px")

kable(tail(data, 10)) %>% 
  kable_styling("striped", full_width = F) %>% 
  scroll_box(width = "800px", height = "500px")

str(data)
```

## Mutating Data
We preprocess the dataset to ensure all variables are in the appropriate format for efficient modeling. We convert all character columns to factors, which ensures categorical variables, such as song title, are handled correctly by the regression models. We also convert the duration from to an integer to properly treat it as a numeric variable.

``` {r}
#mutate our data so character values become factors
data <- data %>%
  mutate_if(is.character, as.factor) %>%
  mutate(Length..Duration. = as.integer(Length..Duration.))
str(data)
```

# Plots

## Plot 1
We create a correlation heatmap to visualize relationships among all numeric variables. The heatmap uses a color gradient to show the strength of the  correlations. It helps us identify which audio features are strongly correlated with popularity and also whether multicollinearity may impact final model predictions.

``` {r Heatmap}
numeric_columns <- data %>%
  dplyr::select(where(is.numeric)) %>%
  dplyr::select(-any_of("ID"))

# Plot de heatmap 
ggcorr(
  numeric_columns, 
  label = FALSE,            
  layout.exp = 1,
  low = "#7F7FFF",          # blauw voor lage correlatie
  mid = "white", 
  high = "#FF4C4C",         # rood voor hoge correlatie
  name = "Correlatie"
) +
  labs(
    title = "Correlatie tussen numerieke variabelen",
    caption = "Rood = sterke correlatie, Blauw = zwak of geen correlatie"
  )

```

## Plot 2
We calculate the average values of danceability, energy, loudness, and popularity by year. A line plot is used to visualize the trends and it allows us to explore how these features evolved over time.

``` {r}
data_long <- data %>%
  dplyr::select(Year, Danceability, Energy, Loudness..dB., Popularity) %>%
  pivot_longer(cols = c(Danceability, Energy, Loudness..dB., Popularity),
               names_to = "Feature",
               values_to = "Value") %>%
  group_by(Year, Feature) %>%
  summarise(Avg_Value = mean(Value, na.rm = TRUE), .groups = "drop")

ggplot(data_long, aes(x = Year, y = Avg_Value, color = Feature)) +
  geom_line(size = 1) +
  labs(title = "Trends of some musical features and popularity over time",
       x = "Year",
       y = "Average Value") +
  theme_minimal()
```

## Plot 3 

# Regression

## Distinguishing the Dataset
We split the dataset into training (70%), validation (15%), and test (15%) sets. We then plot the density of popularity scores across all sets within a single plot to ensure similar distributions. The data partitioning prepares the data for model training and evaluation, while the density plot confirms that the splits are balanced to ensure that model performance metrics are reliable.

``` {r}
# 70% train, 15% validation, 15% test
n <- nrow(data)
train_index <- sample(1:n, size = 0.7 * n)
remaining <- setdiff(1:n, train_index)
valid_index <- sample(remaining, size = 0.5 * length(remaining))
test_index <- setdiff(remaining, valid_index)

data_train <- data[train_index, ]
data_valid <- data[valid_index, ]
data_test  <- data[test_index, ]


# Combine the datasets, adding a column to distinguish them
combined_data <- bind_rows(
  mutate(data_train, Set = "Training"),
  mutate(data_valid, Set = "Validation"),
  mutate(data_test, Set = "Test")
)

# Checking if the distribution of popularity is similar in the three datasets
ggplot(combined_data, aes(x = Popularity, colour = Set)) +
  geom_density(trim = T, linewidth = 1) +
  labs(title = "Popularity Distribution Across Datasets",
       x = "Popularity",
       y = "Frequency") +
  theme_minimal()
```

## Lasso-Regression
We perform Lasso regression to identify the most predictive feature for popularity. It is ideal for our purpose, as it sets insignificant feature coefficients to zero, resulting in a model that showcases the most predictive features. The matrices are prepared by excluding any non-numeric columns and cross-validation then finds the optimal lambda to use for fitting the final model. We have this model make predictions on the validation set and compute the mean squared error to evaluate its performance. The model’s coefficients are displayed in a table, which showcases all the significant predictors when determining a song's popularity.

``` {r}


# Prepare model matrices
x <- model.matrix(Popularity ~ . - Index - Title - Artist - Top.Genre, data_train)[, -1]
y <- data_train$Popularity

x_valid <- model.matrix(Popularity ~ . - Index - Title - Artist - Top.Genre, data_valid)[, -1]
y_valid <- data_valid$Popularity

# Lasso Regression with cross-validation
cv_lasso <- cv.glmnet(x, y, alpha = 1)
best_lambda <- cv_lasso$lambda.min

# Fit final model
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)

# Predict on validation data
pred_valid <- predict(lasso_model, s = best_lambda, newx = x_valid)

# Compute validation MSE
lasso_mse <- mean((y_valid - pred_valid)^2)
lasso_mse



# Extract coefficients from Lasso model
lasso_coef <- coef(lasso_model)

# Convert to data frame
lasso_coef_df <- as.data.frame(as.matrix(lasso_coef))
colnames(lasso_coef_df) <- "Coefficient"

# feature names
lasso_coef_df <- lasso_coef_df %>%
  rownames_to_column(var = "Feature") %>%
  filter(Coefficient != 0) %>%
  arrange(desc(abs(Coefficient)))

# table
kable(lasso_coef_df, caption = "Significant Predictors") %>%
  kable_styling("striped", full_width = F) %>%
  scroll_box(width = "600px", height = "400px")

```

# Conclusion
In this analysis, we looked at which audio features help predict how popular a song is on Spotify. We used the Lasso regression to find out which characteristics matter most. The Lasso model gave us clearer results by picking out the most important features. These included loudness, speechiness, year, danceability, energy, and a few others. Loudness had the biggest positive effect, meaning louder songs tend to be more popular. Speechiness and danceability also had positive impacts, while features like energy and liveness had smaller negative effects. Interestingly, the year had a negative relationship, which might suggest older songs are more popular in this dataset. Overall, we found that a song’s energy, lyrics, and rhythm play a big role in how popular it becomes.

# Contribution
Osaro: Created Plot 1, wrote the Research question and conclusion. Wrote the lasso Regression.
Calvin:
Jair:
