knitr::opts_chunk$set(echo = TRUE)
data <- read_xlsx("data/Spotify-2000.csv")
library(tidyr)
library(readxl)
data <- read_xlsx("data/Spotify-2000.csv")
data <- read_xlsx("aDaV/data/Spotify-2000.csv")
data <- read_xlsx("aDav/data/Spotify-2000.csv")
data <- read_xlsx("aDaV/aDav/data/Spotify-2000.csv")
dir.exists("C:/aDav")
getwd()
data <- read_xlsx("/data/Spotify-2000.csv")
data <- read_xlsx("data/Spotify-2000.csv")
knitr::opts_chunk$set(echo = TRUE)
mse <- function(y_true, y_pred) {
mean((y_true - y_pred)^2)
}
#remove the comment to run the function
mse(1:10, 10:1)
y_true <- Boston$medv
data <- read_xlsx("data/Spotify-2000.xlsx")
data <- read.csv("data/Spotify-2000.csv")
library(tidyr)
library(readr)
data <- read.csv("data/Spotify-2000.csv")
View(data)
summary(data)
head(data)
summary(data)
head(data)
kable(head(data, 10)) %>%
kable_styling("striped", full_width = F) %>%
scroll_box(width = "800px", height = "500px")
suppressWarnings({
library(dplyr)
library(ggplot2)
library(ggpubr)
library(kableExtra)
library(tidyverse)
library(readr)
library(knitr)
library(weathermetrics)
library(gridExtra)
library(plotly)
library(GGally)
library(magrittr)
library(regclass)
library(MASS)
library(pROC)
library(caret)
library(car)
})
data <- read.csv("data/Spotify-2000.csv")
summary(data)
head(data)
kable(head(data, 10)) %>%
kable_styling("striped", full_width = F) %>%
scroll_box(width = "800px", height = "500px")
kable(tail(data, 10)) %>%
kable_styling("striped", full_width = F) %>%
scroll_box(width = "800px", height = "500px")
str(data)
kable(head(data, 10)) %>%
kable_styling("striped", full_width = F) %>%
scroll_box(width = "800px", height = "500px")
kable(tail(data, 10)) %>%
kable_styling("striped", full_width = F) %>%
scroll_box(width = "800px", height = "500px")
str(data)
#mutate our data so character values become factors
data <- data %>%
mutate_if(is.character, as.factor)
str(data)
# Summarizing age statistics by gender
by_age_summary <- data %>%
group_by(Gender) %>%
summarise(
min = min(Age),
max = max(Age),
Q1 = quantile(Age, 0.25),
median = median(Age),
Q3 = quantile(Age, 0.75),
IQR = Q3 - Q1
) %>%
kbl(caption = "Gender Age Statistics") %>%
kable_classic_2(latex_options = c("striped", "hover"), full_width = F, position = "left")
#reading in the libraries
suppressWarnings({
library(dplyr)
library(ggplot2)
library(ggpubr)
library(kableExtra)
library(tidyverse)
library(readr)
library(knitr)
library(weathermetrics)
library(gridExtra)
library(plotly)
library(GGally)
library(magrittr)
library(regclass)
library(MASS)
library(pROC)
library(caret)
library(car)
})
#loading in our dataset
data <- read.csv("sleep_health_and_lifestyle_dataset.csv")
library(caret)
# define the training partition
train_index <- createDataPartition(Boston$medv, p = .7,
list = FALSE,
times = 1)
# split the data using the training partition to obtain training data
boston_train <- Boston[train_index,]
# remainder of the split is the validation and test data (still) combined
boston_val_and_test <- Boston[-train_index,]
# split the remaining 30% of the data in a validation and test set
val_index <- createDataPartition(boston_val_and_test$medv, p = .66,
list = FALSE,
times = 1)
boston_valid <- boston_val_and_test[val_index,]
boston_test  <- boston_val_and_test[-val_index,]
# Outcome of this section is that the data (100%) is split into:
# training (~70%)
# validation (~20%)
# test (~10%)
library(caret)
# define the training partition
train_index <- createDataPartition(Boston$medv, p = .7,
list = FALSE,
times = 1)
# split the data using the training partition to obtain training data
boston_train <- Boston[train_index,]
# remainder of the split is the validation and test data (still) combined
boston_val_and_test <- Boston[-train_index,]
# split the remaining 30% of the data in a validation and test set
val_index <- createDataPartition(boston_val_and_test$medv, p = .66,
list = FALSE,
times = 1)
boston_valid <- boston_val_and_test[val_index,]
boston_test  <- boston_val_and_test[-val_index,]
# Outcome of this section is that the data (100%) is split into:
# training (~70%)
# validation (~20%)
# test (~10%)
set.seed(123)  # Voor reproduceerbaarheid
# Training: 70%
train_index <- createDataPartition(Boston$medv, p = 0.7, list = FALSE)
boston_train <- Boston[train_index,]
boston_val_and_test <- Boston[-train_index,]
# Validatie: 66% van resterende 30% â‰ˆ 20%
val_index <- createDataPartition(boston_val_and_test$medv, p = 0.66, list = FALSE)
boston_valid <- boston_val_and_test[val_index,]
boston_test <- boston_val_and_test[-val_index,]
data %>%
group_by(Top.Genre) %>%
summarise(Avg_Popularity = mean(Popularity, na.rm = TRUE),
Count = n()) %>%
filter(Count > 20) %>%  # filter to more common genres
arrange(desc(Avg_Popularity)) %>%
top_n(10, Avg_Popularity) %>%
ggplot(aes(x = reorder(Top.Genre, Avg_Popularity), y = Avg_Popularity)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 10 Genres by Average Popularity",
x = "Genre", y = "Average Popularity")
data %>%
group_by(Top.Genre) %>%
summarise(Avg_Popularity = mean(Popularity, na.rm = TRUE),
Count = n()) %>%
filter(Count > 20) %>%  # filter to more common genres
arrange(desc(Avg_Popularity)) %>%
top_n(10, Avg_Popularity) %>%
ggplot(aes(x = reorder(Top.Genre, Avg_Popularity), y = Avg_Popularity)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 10 Genres by Average Popularity",
x = "Genre", y = "Average Popularity")
