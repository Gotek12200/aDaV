best_lambda <- cv_lasso$lambda.min
# Fit final model
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
# Predict on validation data
pred_valid <- predict(lasso_model, s = best_lambda, newx = x_valid)
# Compute validation MSE
lasso_mse <- mean((y_valid - pred_valid)^2)
lasso_mse
coef(lasso_model)
# Alleen numerieke variabelen selecteren
numeric_data <- data %>%
select(where(is.numeric)) %>%
select(-Index)  # Index heeft geen inhoudelijke waarde
# Correlatiematrix berekenen
cor_matrix <- round(cor(numeric_data, use = "complete.obs"), 2)
# Custom kleurfunctie: rood voor sterke correlatie, groen voor zwakke
cor_col_fun <- function(value) {
if (abs(value) >= 0.7) {
"background-color:#f4cccc;"  # roodachtig
} else {
"background-color:#d9ead3;"  # groenachtig
}
}
# Opmaak toepassen
kbl(cor_matrix, caption = "Correlatiematrix tussen numerieke variabelen") %>%
kable_styling("striped", full_width = F) %>%
row_spec(0, bold = TRUE) %>%
column_spec(1:ncol(cor_matrix), background = cor_col_fun) %>%
scroll_box(width = "1000px", height = "500px")
knitr::opts_chunk$set(echo = TRUE)
suppressWarnings({
library(dplyr)
library(ggplot2)
library(ggpubr)
library(kableExtra)
library(tidyverse)
library(readr)
library(knitr)
library(weathermetrics)
library(gridExtra)
library(plotly)
library(GGally)
library(magrittr)
library(regclass)
library(MASS)
library(pROC)
library(caret)
library(car)
})
# Set seed for reproducibility
set.seed(123)
data <- read.csv("data/Spotify-2000.csv")
kable(head(data, 10)) %>%
kable_styling("striped", full_width = F) %>%
scroll_box(width = "800px", height = "500px")
kable(tail(data, 10)) %>%
kable_styling("striped", full_width = F) %>%
scroll_box(width = "800px", height = "500px")
str(data)
#mutate our data so character values become factors
data <- data %>%
mutate_if(is.character, as.factor) %>%
mutate(Length..Duration. = as.integer(Length..Duration.))
str(data)
data %>%
group_by(Top.Genre) %>%
summarise(Avg_Popularity = mean(Popularity, na.rm = TRUE),
Count = n()) %>%
filter(Count > 10) %>%
arrange(desc(Avg_Popularity)) %>%
top_n(10, Avg_Popularity) %>%
ggplot(aes(x = reorder(Top.Genre, Avg_Popularity), y = Avg_Popularity)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 10 Genres by Average Popularity",
x = "Genre", y = "Average Popularity")
# Alleen numerieke variabelen selecteren
numeric_data <- data %>%
select(where(is.numeric)) %>%
select(-Index)  # Index heeft geen inhoudelijke waarde
# Correlatiematrix berekenen
cor_matrix <- round(cor(numeric_data, use = "complete.obs"), 2)
# Custom kleurfunctie: rood voor sterke correlatie, groen voor zwakke
cor_col_fun <- function(value) {
if (abs(value) >= 0.7) {
"background-color:#f4cccc;"  # roodachtig
} else {
"background-color:#d9ead3;"  # groenachtig
}
}
# Opmaak toepassen
kbl(cor_matrix, caption = "Correlatiematrix tussen numerieke variabelen") %>%
kable_styling("striped", full_width = F) %>%
row_spec(0, bold = TRUE) %>%
column_spec(1:ncol(cor_matrix), background = cor_col_fun) %>%
scroll_box(width = "1000px", height = "500px")
#impact of year of popularity. How much of the popularity is nostalgia
data %>%
mutate(Demi.Decade = (Year %/% 5) * 5) %>%
group_by(Demi.Decade) %>%
summarise(Avg_Popularity = mean(Popularity, na.rm = TRUE)) %>%
ggplot(aes(x = Demi.Decade, y = Avg_Popularity)) +
geom_col()
data %>%
group_by(Year) %>%
summarise(Avg_Popularity = mean(Popularity, na.rm = TRUE)) %>%
ggplot(aes(x = Year, y = Avg_Popularity)) +
geom_col()
?group_by
ggplot(data, aes(y = Popularity, x = Year)) +
geom_hex()
?geom_hex
# Doing a basic data partition. We might want to do k-fold cross validation later instead
train_index <- createDataPartition(data$Popularity, p = .7,
list = FALSE,
times = 1)
data_train <- data[train_index,]
data_val_and_test <- data[-train_index,]
val_index <- createDataPartition(data_val_and_test$Popularity, p = .66,
list = FALSE,
times = 1)
data_valid <- data_val_and_test[val_index,]
data_test  <- data_val_and_test[-val_index,]
dataset_partitions <- data.frame(
"Dataset" = c("Full Dataset", "Validation and Test Set", "Training Set", "Validation Set", "Test Set"),
"N of Obs" = c(nrow(data), nrow(data_val_and_test), nrow(data_train), nrow(data_valid), nrow(data_test)),
"% Split" = c(
sprintf("%%%s", round(((nrow(data) / nrow(data))*100), 2)),
sprintf("%%%s", round(((nrow(data_val_and_test) / nrow(data))*100), 2)),
sprintf("%%%s", round(((nrow(data_train) / nrow(data))*100), 2)),
sprintf("%%%s", round(((nrow(data_valid) / nrow(data))*100), 2)),
sprintf("%%%s", round(((nrow(data_test) / nrow(data))*100), 2))
)
)
dataset_partitions %>%
kbl(caption = "Dataset Partitions") %>%
kable_classic(full_width = F, html_font = "Cambria")
# Combine the datasets, adding a column to distinguish them
combined_data <- bind_rows(
mutate(data_train, Set = "Training"),
mutate(data_valid, Set = "Validation"),
mutate(data_test, Set = "Test")
)
# Checking if the distribution of popularity is similar in the three datasets
ggplot(combined_data, aes(x = Popularity, colour = Set)) +
geom_density(trim = T, linewidth = 1) +
labs(title = "Popularity Distribution Across Datasets",
x = "Popularity",
y = "Frequency") +
theme_minimal()
# Creating/importing necessary functions
generate_formulas <- function(p, x_vars, y_var) {
if (p %% 1 != 0)           stop("Input an integer n")
if (p > length(x_vars))    stop("p should be smaller than number of vars")
if (!is.character(x_vars)) stop("x_vars should be a character vector")
if (!is.character(y_var))  stop("y_vars should be character type")
apply(combn(x_vars, p), 2, function(vars) {
paste0(y_var, " ~ ", paste(vars, collapse = " + "))
})
}
lm_mse <- function(formula, train_data, valid_data) {
y_name <- as.character(formula)[2]
y_true <- valid_data[[y_name]]
lm_fit <- lm(formula, train_data)
y_pred <- predict(lm_fit, newdata = valid_data)
mean((y_true - y_pred)^2)
}
# I made a neat function for finding the best set of predictors
pred_set <- function(pred_count, pred_names, out_name, train_data, valid_data) {
formulas <- generate_formulas(pred_count, pred_names, out_name)
mses <- rep(0, length(formulas))
for (i in 1:length(formulas)) {
mses[i] <- lm_mse(as.formula(formulas[i]), train_data, valid_data)
}
list(
set = formulas[which.min(mses)],
mse = min(mses)
)
}
# Testing for the best set of predictors without including year
pred_names <- setdiff(colnames(data), c("Index", "Title", "Artist", "Top.Genre", "Year", "Popularity"))
pred_set(1, pred_names, "Popularity", data_train, data_valid)
pred_set(2, pred_names, "Popularity", data_train, data_valid)
pred_set(3, pred_names, "Popularity", data_train, data_valid)
pred_set(4, pred_names, "Popularity", data_train, data_valid)
# Testing for the best set of predictors INCLUDING year
pred_names_y <- setdiff(colnames(data), c("Index", "Title", "Artist", "Top.Genre", "Popularity"))
pred_set(1, pred_names_y, "Popularity", data_train, data_valid)
pred_set(2, pred_names_y, "Popularity", data_train, data_valid)
pred_set(3, pred_names_y, "Popularity", data_train, data_valid)
pred_set(4, pred_names_y, "Popularity", data_train, data_valid)
library(glmnet)
# Prepare model matrices
x <- model.matrix(Popularity ~ . - Index - Title - Artist - Top.Genre, data_train)[, -1]
y <- data_train$Popularity
x_valid <- model.matrix(Popularity ~ . - Index - Title - Artist - Top.Genre, data_valid)[, -1]
y_valid <- data_valid$Popularity
# Lasso Regression with cross-validation
cv_lasso <- cv.glmnet(x, y, alpha = 1)
best_lambda <- cv_lasso$lambda.min
# Fit final model
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
# Predict on validation data
pred_valid <- predict(lasso_model, s = best_lambda, newx = x_valid)
# Compute validation MSE
lasso_mse <- mean((y_valid - pred_valid)^2)
lasso_mse
coef(lasso_model)
# Selecteer enkel numerieke kolommen
numeric_data <- data %>%
dplyr::select(where(is.numeric))
# Bereken correlatiematrix
cor_matrix <- round(cor(numeric_data, use = "complete.obs"), 2)
# Maak er een dataframe van voor styling
cor_df <- as.data.frame(cor_matrix)
# Definieer kleurfunctie: rood bij sterke correlatie (|r| >= 0.7)
color_cor <- function(val) {
if (abs(val) >= 0.7) {
"background-color:#f4cccc;"  # rood
} else {
"background-color:#d9ead3;"  # groen
}
}
# Maak een matrix met kleurtjes
cell_colors <- matrix(sapply(cor_matrix, color_cor),
nrow = nrow(cor_matrix),
ncol = ncol(cor_matrix),
dimnames = dimnames(cor_matrix))
# Tabel renderen met kleur op elke cel
kbl(cor_df, escape = FALSE, caption = "Correlatiematrix tussen numerieke variabelen") %>%
kable_styling("striped", full_width = F) %>%
row_spec(0, bold = TRUE) %>%
column_spec(1:ncol(cor_df), background = cell_colors) %>%
scroll_box(width = "1000px", height = "500px")
library(dplyr)
library(kableExtra)
# Select only numeric columns
numeric_data <- data %>%
dplyr::select(where(is.numeric))
# Compute correlation matrix
cor_matrix <- round(cor(numeric_data, use = "complete.obs"), 2)
# Convert matrix to data frame and add row names as a column
cor_df <- as.data.frame(cor_matrix)
cor_df <- tibble::rownames_to_column(cor_df, "Variable")
# Apply color formatting to each cell
cor_df_colored <- cor_df %>%
mutate(across(where(is.numeric), ~cell_spec(.x,
background = ifelse(abs(.x) >= 0.7, "#f4cccc", "#d9ead3")
)))
# Show colored table
kbl(cor_df_colored, escape = FALSE, caption = "Correlatiematrix tussen numerieke variabelen (rood = sterk, groen = zwak)") %>%
kable_styling("striped", full_width = F) %>%
scroll_box(width = "1000px", height = "500px")
library(GGally)
library(dplyr)
# Selecteer numerieke kolommen (zonder ID als die er is)
numeric_columns <- data %>%
dplyr::select(where(is.numeric)) %>%
dplyr::select(-any_of("ID"))  # veilig: alleen als "ID" bestaat
# Plot de correlatie heatmap
ggcorr(
numeric_columns,
label = TRUE,
label_round = 2,
label_alpha = TRUE,
layout.exp = 1,
hjust = 0.75,
size = 3,
low = "#d9ead3",   # groen
high = "#f4cccc",  # rood
mid = "white"
) +
labs(
title = "Correlatie tussen numerieke variabelen",
caption = "Rood = sterke correlatie, Groen = zwak of geen correlatie"
)
library(GGally)
library(dplyr)
# Selecteer numerieke kolommen (ID veilig uitsluiten als die er is)
numeric_columns <- data %>%
dplyr::select(where(is.numeric)) %>%
dplyr::select(-any_of("ID"))
# Plot de heatmap zonder labels
ggcorr(
numeric_columns,
label = FALSE,            # geen cijfers tonen
layout.exp = 1,
low = "#d9ead3",          # groen voor lage correlatie
mid = "white",
high = "#f4cccc",         # rood voor hoge correlatie
name = "Correlatie"
) +
labs(
title = "Correlatie tussen numerieke variabelen",
caption = "Rood = sterke correlatie, Groen = zwak of geen correlatie"
)
library(GGally)
library(dplyr)
# Selecteer numerieke kolommen (ID veilig uitsluiten als die er is)
numeric_columns <- data %>%
dplyr::select(where(is.numeric)) %>%
dplyr::select(-any_of("ID"))
# Plot de heatmap zonder labels
ggcorr(
numeric_columns,
label = FALSE,
layout.exp = 1,
low = "#7FFF7F",          # groen voor lage correlatie
mid = "white",
high = "#FF4C4C",         # rood voor hoge correlatie
name = "Correlatie"
) +
labs(
title = "Correlatie tussen numerieke variabelen",
caption = "Rood = sterke correlatie, Groen = zwak of geen correlatie"
)
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
# Prepare model matrices
x <- model.matrix(Popularity ~ . - Index - Title - Artist - Top.Genre, data_train)[, -1]
y <- data_train$Popularity
x_valid <- model.matrix(Popularity ~ . - Index - Title - Artist - Top.Genre, data_valid)[, -1]
y_valid <- data_valid$Popularity
# Lasso Regression with cross-validation
cv_lasso <- cv.glmnet(x, y, alpha = 1)
best_lambda <- cv_lasso$lambda.min
# Fit final model
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
# Predict on validation data
pred_valid <- predict(lasso_model, s = best_lambda, newx = x_valid)
# Compute validation MSE
lasso_mse <- mean((y_valid - pred_valid)^2)
lasso_mse
coef(lasso_model)
knitr::opts_chunk$set(echo = TRUE)
suppressWarnings({
library(dplyr)
library(ggplot2)
library(ggpubr)
library(kableExtra)
library(tidyverse)
library(readr)
library(knitr)
library(weathermetrics)
library(gridExtra)
library(plotly)
library(GGally)
library(magrittr)
library(regclass)
library(MASS)
library(pROC)
library(caret)
library(car)
})
# Set seed for reproducibility
set.seed(123)
data <- read.csv("data/Spotify-2000.csv")
kable(head(data, 10)) %>%
kable_styling("striped", full_width = F) %>%
scroll_box(width = "800px", height = "500px")
kable(tail(data, 10)) %>%
kable_styling("striped", full_width = F) %>%
scroll_box(width = "800px", height = "500px")
str(data)
#mutate our data so character values become factors
data <- data %>%
mutate_if(is.character, as.factor) %>%
mutate(Length..Duration. = as.integer(Length..Duration.))
str(data)
data %>%
group_by(Top.Genre) %>%
summarise(Avg_Popularity = mean(Popularity, na.rm = TRUE),
Count = n()) %>%
filter(Count > 10) %>%
arrange(desc(Avg_Popularity)) %>%
top_n(10, Avg_Popularity) %>%
ggplot(aes(x = reorder(Top.Genre, Avg_Popularity), y = Avg_Popularity)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 10 Genres by Average Popularity",
x = "Genre", y = "Average Popularity")
library(GGally)
library(dplyr)
numeric_columns <- data %>%
dplyr::select(where(is.numeric)) %>%
dplyr::select(-any_of("ID"))
# Plot de heatmap zonder labels
ggcorr(
numeric_columns,
label = FALSE,
layout.exp = 1,
low = "#7FFF7F",          # groen voor lage correlatie
mid = "white",
high = "#FF4C4C",         # rood voor hoge correlatie
name = "Correlatie"
) +
labs(
title = "Correlatie tussen numerieke variabelen",
caption = "Rood = sterke correlatie, Groen = zwak of geen correlatie"
)
#impact of year of popularity. How much of the popularity is nostalgia
data %>%
mutate(Demi.Decade = (Year %/% 5) * 5) %>%
group_by(Demi.Decade) %>%
summarise(Avg_Popularity = mean(Popularity, na.rm = TRUE)) %>%
ggplot(aes(x = Demi.Decade, y = Avg_Popularity)) +
geom_col()
data %>%
group_by(Year) %>%
summarise(Avg_Popularity = mean(Popularity, na.rm = TRUE)) %>%
ggplot(aes(x = Year, y = Avg_Popularity)) +
geom_col()
?group_by
ggplot(data, aes(y = Popularity, x = Year)) +
geom_hex()
?geom_hex
# Doing a basic data partition. We might want to do k-fold cross validation later instead
train_index <- createDataPartition(data$Popularity, p = .7,
list = FALSE,
times = 1)
data_train <- data[train_index,]
data_val_and_test <- data[-train_index,]
val_index <- createDataPartition(data_val_and_test$Popularity, p = .66,
list = FALSE,
times = 1)
data_valid <- data_val_and_test[val_index,]
data_test  <- data_val_and_test[-val_index,]
dataset_partitions <- data.frame(
"Dataset" = c("Full Dataset", "Validation and Test Set", "Training Set", "Validation Set", "Test Set"),
"N of Obs" = c(nrow(data), nrow(data_val_and_test), nrow(data_train), nrow(data_valid), nrow(data_test)),
"% Split" = c(
sprintf("%%%s", round(((nrow(data) / nrow(data))*100), 2)),
sprintf("%%%s", round(((nrow(data_val_and_test) / nrow(data))*100), 2)),
sprintf("%%%s", round(((nrow(data_train) / nrow(data))*100), 2)),
sprintf("%%%s", round(((nrow(data_valid) / nrow(data))*100), 2)),
sprintf("%%%s", round(((nrow(data_test) / nrow(data))*100), 2))
)
)
dataset_partitions %>%
kbl(caption = "Dataset Partitions") %>%
kable_classic(full_width = F, html_font = "Cambria")
# Combine the datasets, adding a column to distinguish them
combined_data <- bind_rows(
mutate(data_train, Set = "Training"),
mutate(data_valid, Set = "Validation"),
mutate(data_test, Set = "Test")
)
# Checking if the distribution of popularity is similar in the three datasets
ggplot(combined_data, aes(x = Popularity, colour = Set)) +
geom_density(trim = T, linewidth = 1) +
labs(title = "Popularity Distribution Across Datasets",
x = "Popularity",
y = "Frequency") +
theme_minimal()
# Creating/importing necessary functions
generate_formulas <- function(p, x_vars, y_var) {
if (p %% 1 != 0)           stop("Input an integer n")
if (p > length(x_vars))    stop("p should be smaller than number of vars")
if (!is.character(x_vars)) stop("x_vars should be a character vector")
if (!is.character(y_var))  stop("y_vars should be character type")
apply(combn(x_vars, p), 2, function(vars) {
paste0(y_var, " ~ ", paste(vars, collapse = " + "))
})
}
lm_mse <- function(formula, train_data, valid_data) {
y_name <- as.character(formula)[2]
y_true <- valid_data[[y_name]]
lm_fit <- lm(formula, train_data)
y_pred <- predict(lm_fit, newdata = valid_data)
mean((y_true - y_pred)^2)
}
# I made a neat function for finding the best set of predictors
pred_set <- function(pred_count, pred_names, out_name, train_data, valid_data) {
formulas <- generate_formulas(pred_count, pred_names, out_name)
mses <- rep(0, length(formulas))
for (i in 1:length(formulas)) {
mses[i] <- lm_mse(as.formula(formulas[i]), train_data, valid_data)
}
list(
set = formulas[which.min(mses)],
mse = min(mses)
)
}
# Testing for the best set of predictors without including year
pred_names <- setdiff(colnames(data), c("Index", "Title", "Artist", "Top.Genre", "Year", "Popularity"))
pred_set(1, pred_names, "Popularity", data_train, data_valid)
pred_set(2, pred_names, "Popularity", data_train, data_valid)
pred_set(3, pred_names, "Popularity", data_train, data_valid)
pred_set(4, pred_names, "Popularity", data_train, data_valid)
# Testing for the best set of predictors INCLUDING year
pred_names_y <- setdiff(colnames(data), c("Index", "Title", "Artist", "Top.Genre", "Popularity"))
pred_set(1, pred_names_y, "Popularity", data_train, data_valid)
pred_set(2, pred_names_y, "Popularity", data_train, data_valid)
pred_set(3, pred_names_y, "Popularity", data_train, data_valid)
pred_set(4, pred_names_y, "Popularity", data_train, data_valid)
library(glmnet)
# Prepare model matrices
x <- model.matrix(Popularity ~ . - Index - Title - Artist - Top.Genre, data_train)[, -1]
y <- data_train$Popularity
x_valid <- model.matrix(Popularity ~ . - Index - Title - Artist - Top.Genre, data_valid)[, -1]
y_valid <- data_valid$Popularity
# Lasso Regression with cross-validation
cv_lasso <- cv.glmnet(x, y, alpha = 1)
best_lambda <- cv_lasso$lambda.min
# Fit final model
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
# Predict on validation data
pred_valid <- predict(lasso_model, s = best_lambda, newx = x_valid)
# Compute validation MSE
lasso_mse <- mean((y_valid - pred_valid)^2)
lasso_mse
coef(lasso_model)
